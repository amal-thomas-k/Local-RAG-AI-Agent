## Local AI Agent with Ollama, LangChain and SingleStore
In this tutorial, you’ll learn how to build a local Retrieval-Augmented Generation (RAG) AI agent.

### This agent will run entirely on your machine and leverage:
- Ollama for open-source LLMs and embeddings
- LangChain for orchestration
- SingleStore as the vector store
  
[Signup to SingleStore for free!](https://portal.singlestore.com/intention/cloud?utm_medium=referral&utm_source=pavan&utm_term=github&utm_content=AIAgent)
  
By the end of this tutorial, you’ll have a fully working Q+A system powered by your local data and models.

### Prerequisites
Ensure the following are installed and configured:
- Docker (running)
- Python 3.9+
- Ollama installed and working

For more detailed information, read the [complete article](https://www.singlestore.com/blog/build-a-local-ai-agent-python-ollama-langchain-singlestore/). 
